---
title: "Not enough memory? HDF5, a data format for single cell RNAseq data"
output: github_document
date: "2024-02-23"
editor_options: 
  chunk_output_type: console
---

Single cell RNAseq data (scRNAseq) is substantially more complex than the previous era of bulk gene expression analysis, and part of that complexity is managing high memory operations on large data structures: 1 million observations (e.g. cells) x thousands (genes) or millions (CpGs) of observations. 

What tools exist for dealing with this high dimensional type of data? In R, there are data structures, and tools for reading/writing, computing, and visualizing scRNAseq data. Today, I focus on the case where the data is too large or cumbersome to hold in memory (>1Gb - > 50 Gb), which is becoming increasingly common as scRNAseq datasets become more numerous and larger in size.

But considerations for memory when working with such large data must also include the specific tasks a user wants to perform. For example, computing matrix operations, like transpose, arithmetic, etc, are going to be very different than discussions on dataset integration, cell type annotation, and clustering. 

Today, I focus on the common task of analyzing 1 gene in R at a time, without needing to load the entire scRNAseq gene expression matrix into memory. 

I choose this as my focus, because I think this is a common connection point  from the very specific single cell universe of data structures and tools, to more general purpose data science toolkit e.g. tidyverse, data.table. I believe this is a very useful task, that is not well explored in the scRNAseq R data science world. This enables for example, very lightweight shiny-based web apps, that won't error out from memory limits. 

# start point

The starting point for this task is that we have a sparse count matrix, maybe a `Seurat` or `SingleCellExperiment` object that I want to make accessible via a shiny app to stakeholders. I can write an app that just simply works with these objects directly, but that would involve loading the several gigabytes of data into memory.

I was looking for ways to circumvent loading entire count matrices (the main memory-hungry culprit) into R, and this post is about that exploration.

The outline for this post is that: there are some great "on-disk" streaming options, where essentially you open a *connection* to a file, then only subsets of the data file that you request are actually read into memory. 

# the plan

Actually, there is already a great memory-efficient solution out there called ShinyCell, which utilizes the HDF5 format. This doesn't work perfectly for me though, because I have my own app functions and framework that I want to apply. I am just interested in the memory-efficient data read functionality.

So I will compare shinycell's rather manual h5 strategy to some other options. The other option included here is `HDF5Array` + `delayedArray`, which are some Bioconductor options specifically for Bioconductor-specific data structures, like scRNAseq.

# data formats for single cell

Some common formats for single cell, with drawbacks / considerations described:

`.rds` - smaller on-disk footprint, quicker load, but specific to R, cannot read subsets of data; must read entire data into memory

`csv` - a universal format, can speed up with `vroom`, one challenge is to write a sparse count matrix to dense representation as csv 

one option I have not explored is using csvs could be to write sparse matrix to disk; then, after reading can coerce to dense. Coercion will add some overhead.

`.h5ad` large on-disk footprint but allows reading subsets of data into memory. The major drawback is that it is specific to python. Although there are tools to convert into R workflows, like `basilisk` and `zellkonverter`. But these don't take advantage of the hdf5 on-disk memory savings.





How easy is it to use hdf5 backed single cell gene expression matrices? How much will HDF5 improve memory / speed in common single cell queries? I hope to explore these questions in this post, by using an example use case of simply reading 1 gene from a file.

I chose this as "reading 1 gene" is a common task in my analysis, pipelines, and in web apps. Most solutions for memory-optimized single cell analysis is also optimized for working with the entire 22,000 genes x 1 million cells data matrices. 

# key packages for this post

For HDF5, there are a couple of general purpose options in R: [hdf5r](https://cran.r-project.org/web/packages/hdf5r/index.html), [rhdf5](https://bioconductor.org/packages/release/bioc/html/rhdf5.html). Then there is the bioconductor package [HDF5Array](https://bioconductor.org/packages/release/bioc/html/HDF5Array.html) which uses `hdf5r` in backend to work with bioconductor data structures specifically.

I also recently came across [shinycell](https://github.com/SGDDNB/ShinyCell), a recently released R package that has a manual implementation of HDF5 to create a low memory shiny app for single cell data. I adapt shinycell's code for my manual approach.


# other packages

`HDF5Array` hdf5 read write dense/sparse matrices
`hdf5r` hdf5 r implementation
`scRNAseq` to access example scRNAseq dataset
`tidyverse` `glue` `gt` general purpose data wrangling
`tictoc` `bench` timing

```{r, message = FALSE, warning = FALSE}
library(HDF5Array)
library(hdf5r)
library(scRNAseq)
library(tidyverse)
library(tictoc)
library(withr)
library(bench)
library(gt)
library(glue)
```

```{r}
sce <- ZilionisLungData()
counts <- sce@assays@data$counts[,]
```

# manual hdf5

There are couple of R implementations of hdf5 for single cell gene expression data:

But these are structured for single cell experiment / seurat / anndata objects,
which at minimum contain counts matrix, but can also include multiple assays,
cell metadata, projections, and other information.

Here I make an attempt to create my own hdf5 structure for taking a dgc matrix
as input. 

# saving dgc matrix manually with hdf5r

Here I litter the function with tictoc::tic and tictoc::toc to understand how
long overall and each step takes.

The strategy here is to write actually a dense representation of the sparase 
matrix.

The alternative would be to write a sparse representation which would be faster
to write (less data), and then convert to dense on the read in endpoints. But 
here the goal is to be able to read fast - the writing can be slow, so we but
the processing (sparse -> dense) on the write end.


```{r}
file_h5 <- here::here('data','counts.h5')
if (file.exists(file_h5)) file.remove(file_h5)

write_dgc_to_h5 <- function(dgc, file, chunk_size = 500) {
  
  # open h5 connection and groups
  h5 <- H5File$new(file, mode = "w")
  on.exit(h5$close_all())
  h5_grp <- h5$create_group("grp")
  h5_grp_data <- h5_grp$create_dataset(
    "data",  
    dtype = h5types$H5T_NATIVE_FLOAT,
    space = H5S$new("simple", dims = dim(dgc), maxdims = dim(dgc)),
    chunk_dims = c(1, ncol(dgc))
  )
  
  tic('total')
  for (i in 1:floor((nrow(dgc) - 8)/chunk_size)) {
    
    index_start <- ((i - 1) * chunk_size) + 1
    index_end <- i * chunk_size
    tic(glue::glue('loop {i}, rows {index_start}:{index_end}'))
    
    h5_grp_data[index_start:index_end, ] <- dgc[index_start:index_end,] |> 
      as.matrix()
    toc(log = TRUE)
  }
  
  # final group
  index_start <- i*chunk_size + 1
  index_end <- nrow(dgc)
  
  tic(glue::glue('final loop, rows {index_start}:{index_end}'))
  h5_grp_data[index_start:index_end, ] <- as.matrix(dgc[index_start:index_end, ])
  toc(log = TRUE)
  
  # add rownames and colnames
  h5_grp[['rownames']] <- rownames(dgc)
  h5_grp[['colnames']] <- colnames(dgc)
  toc()
}
write_dgc_to_h5(counts, file_h5, chunk_size = 1000)
```

# read 1 gene at a time

```{r}
read_gene <- function(gene, file_h5) {
  #tic('whole thing')
  stopifnot(file.exists(file_h5))
  # open connections
  #tic('open')
  h5 <- H5File$new(
    file_h5, 
    mode = "r")
  
  #on.exit(h5$close_all())
  
  h5_data <- h5[['grp']][['data']]
  h5_rownames <- h5[['grp']][['rownames']]
  h5_colnames <- h5[['grp']][['colnames']]
  
  #toc()
  #tic('read gene')
  ind <- str_which(h5_rownames[], gene)
  #ind <- 18871
  #print(ind)
  gene <- h5_data[ind,]
  #toc()
  
  #tic('set name')
  gene <- setNames(gene, nm = h5_colnames[])
  #toc()
  
  #tic('close')
  h5$close_all()
  #toc()
  #toc()
  return(gene) 
}


gene <- read_gene('AC006486.2', file_h5)
```

# HDF5array

Bioconductor has the HDF5array R package that supports writing / loading dense and sparse matrices from .h5 files.

Let's see how this compares to my manual implementation.

First write to disk using `HDF5Array::writeHDF5Array`

```{r}
file_h5array <- here::here('data', 'HDF5array.h5')
if (file.exists(file_h5array)) file.remove(file_h5array)
tic();HDF5Array::writeHDF5Array(
  DelayedArray(counts), 
  file_h5array, as.sparse = FALSE, name = 'full', with.dimnames = TRUE);toc()
h5ls(file_h5array)


hf5 <- HDF5Array(file_h5array,  name = 'full', as.sparse = TRUE)
hf5['AC006486.2',] |>  head()

showtree(hf5)
showtree(hf5[2,])
class(hf5)
is(hf5, 'DelayedMatrix')
```


# compare HDF5array vs diy implementation

Compare performance

```{r}
bench_read <- bench::mark(
  hf5['AC006486.2',],
  read_gene('AC006486.2', file_h5),
  counts['AC006486.2',]
)  

summary(bench_read) |> select(-memory, -result, -time, -gc) |>  gt()
bench_read |>  autoplot(type = 'jitter')
```

HDF5array slower than my manual method

Fastest is holding in memory, but not by that much.

# singlecellexperiment

```{r}
sce_h5 <- SingleCellExperiment(assays = list(counts = hf5))
object.size(sce_h5) |>  print(units = 'auto')
object.size(counts) |>  print(units = 'auto')

bench::mark(
  sce_h5['AC006486.2',]
)
```






